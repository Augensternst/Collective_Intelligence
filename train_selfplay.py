import os
import numpy as np
import gymnasium as gym
from gymnasium import spaces
from stable_baselines3 import PPO
from pikazoo import pikazoo_v0
from pikazoo.wrappers import SimplifyAction
import random


class SelfPlayGymWrapper(gym.Env):
    """è‡ªæˆ‘å¯¹æˆ˜çš„ Gymnasium ç¯å¢ƒåŒ…è£…å™¨"""

    def __init__(self, model_path=None):
        super().__init__()

        # åˆ›å»ºåŸºç¡€ç¯å¢ƒ
        self.base_env = pikazoo_v0.env(winning_score=15, render_mode=None)
        self.base_env = SimplifyAction(self.base_env)

        # è®¾ç½®ç©ºé—´
        self.action_space = self.base_env.action_space("player_1")
        self.observation_space = self.base_env.observation_space("player_1")

        # è‡ªæˆ‘å¯¹æˆ˜ç›¸å…³å±æ€§
        self.current_player = "player_1"
        self.opponent_model = None
        self.episode_count = 0
        self.last_obs = None

        # åŠ è½½å¯¹æ‰‹æ¨¡å‹
        if model_path and os.path.exists(model_path + ".zip"):
            try:
                self.opponent_model = PPO.load(model_path)
                print(f"æˆåŠŸåŠ è½½å¯¹æ‰‹æ¨¡å‹: {model_path}")
            except Exception as e:
                print(f"åŠ è½½å¯¹æ‰‹æ¨¡å‹å¤±è´¥: {e}")

    def reset(self, seed=None, options=None):
        if seed is not None:
            np.random.seed(seed)
            random.seed(seed)

        obs, infos = self.base_env.reset()

        # éšæœºé€‰æ‹©è®­ç»ƒçš„ç©å®¶ï¼ˆå¢åŠ è®­ç»ƒçš„å¤šæ ·æ€§ï¼‰
        self.current_player = random.choice(["player_1", "player_2"])
        self.episode_count += 1
        self.last_obs = obs

        return obs[self.current_player], infos[self.current_player]

    def step(self, action):
        if not self.base_env.agents:
            # æ¸¸æˆå·²ç»“æŸ
            return np.zeros(self.observation_space.shape), 0, True, False, {}

        actions = {}

        # å½“å‰è®­ç»ƒçš„æ™ºèƒ½ä½“åŠ¨ä½œ
        actions[self.current_player] = action

        # å¯¹æ‰‹æ™ºèƒ½ä½“åŠ¨ä½œ
        other_player = "player_2" if self.current_player == "player_1" else "player_1"

        if self.opponent_model is not None and other_player in self.last_obs:
            try:
                # ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ä½œä¸ºå¯¹æ‰‹
                opponent_obs = self.last_obs[other_player]
                opponent_action, _ = self.opponent_model.predict(
                    opponent_obs,
                    deterministic=False
                )
                actions[other_player] = opponent_action
            except Exception as e:
                # å¦‚æœæ¨¡å‹é¢„æµ‹å¤±è´¥ï¼Œä½¿ç”¨éšæœºåŠ¨ä½œ
                actions[other_player] = self.base_env.action_space(other_player).sample()
        else:
            # ä½¿ç”¨éšæœºç­–ç•¥ä½œä¸ºå¯¹æ‰‹
            actions[other_player] = self.base_env.action_space(other_player).sample()

        # æ‰§è¡ŒåŠ¨ä½œ
        obs, rewards, terms, truncs, infos = self.base_env.step(actions)
        self.last_obs = obs

        # æ£€æŸ¥æ¸¸æˆçŠ¶æ€
        if not self.base_env.agents:
            terminated = True
            current_obs = np.zeros(self.observation_space.shape)
            current_reward = rewards.get(self.current_player, 0)
            current_info = {}
        else:
            terminated = terms.get(self.current_player, False)
            current_obs = obs.get(self.current_player, np.zeros(self.observation_space.shape))
            current_reward = rewards.get(self.current_player, 0)
            current_info = infos.get(self.current_player, {})

        truncated = truncs.get(self.current_player, False)

        return current_obs, current_reward, terminated, truncated, current_info

    def close(self):
        self.base_env.close()

    def update_opponent_model(self, model_path):
        """æ›´æ–°å¯¹æ‰‹æ¨¡å‹"""
        if os.path.exists(model_path + ".zip"):
            try:
                self.opponent_model = PPO.load(model_path)
                print(f"å¯¹æ‰‹æ¨¡å‹å·²æ›´æ–°: {model_path}")
            except Exception as e:
                print(f"æ›´æ–°å¯¹æ‰‹æ¨¡å‹å¤±è´¥: {e}")


def train_selfplay():
    """è‡ªæˆ‘å¯¹æˆ˜è®­ç»ƒ"""
    print("å¼€å§‹è‡ªæˆ‘å¯¹æˆ˜è®­ç»ƒ...")

    # åˆ›å»ºæ£€æŸ¥ç‚¹ç›®å½•
    os.makedirs("selfplay_checkpoints", exist_ok=True)

    # åˆ›å»ºåˆå§‹ç¯å¢ƒï¼ˆæ²¡æœ‰å¯¹æ‰‹æ¨¡å‹ï¼‰
    env = SelfPlayGymWrapper()

    print(f"è§‚å¯Ÿç©ºé—´: {env.observation_space}")
    print(f"åŠ¨ä½œç©ºé—´: {env.action_space}")

    # åˆ›å»ºåˆå§‹æ¨¡å‹
    model = PPO(
        "MlpPolicy",
        env,
        verbose=1,
        learning_rate=3e-4,
        n_steps=2048,
        batch_size=64,
        n_epochs=10,
        gamma=0.99,
        tensorboard_log="./pika_selfplay_tb/"
    )

    # è®­ç»ƒè¿­ä»£
    total_iterations = 20
    timesteps_per_iteration = 100000  # æ¯æ¬¡è¿­ä»£çš„è®­ç»ƒæ­¥æ•°

    try:
        for iteration in range(total_iterations):
            print(f"\n=== è‡ªæˆ‘å¯¹æˆ˜è¿­ä»£ {iteration + 1}/{total_iterations} ===")

            # å¦‚æœæœ‰ä¹‹å‰çš„æ¨¡å‹ï¼Œæ›´æ–°å¯¹æ‰‹
            if iteration > 0:
                prev_model_path = f"selfplay_checkpoints/pikazoo_selfplay_iter_{iteration - 1}"
                env.update_opponent_model(prev_model_path)
                print(f"ä½¿ç”¨è¿­ä»£ {iteration} çš„æ¨¡å‹ä½œä¸ºå¯¹æ‰‹")
            else:
                print("ä½¿ç”¨éšæœºç­–ç•¥ä½œä¸ºå¯¹æ‰‹")

            # è®­ç»ƒå½“å‰æ¨¡å‹
            print(f"å¼€å§‹è®­ç»ƒ {timesteps_per_iteration} æ­¥...")
            model.learn(
                total_timesteps=timesteps_per_iteration,
                reset_num_timesteps=False,
                tb_log_name=f"selfplay_iter_{iteration}",
                progress_bar=True
            )

            # ä¿å­˜å½“å‰è¿­ä»£çš„æ¨¡å‹
            model_path = f"selfplay_checkpoints/pikazoo_selfplay_iter_{iteration}"
            model.save(model_path)
            print(f"æ¨¡å‹å·²ä¿å­˜: {model_path}")

            # æ¯éš”å‡ æ¬¡è¿­ä»£è¿›è¡Œä¸€æ¬¡å¿«é€Ÿæµ‹è¯•
            if (iteration + 1) % 3 == 0:
                print(f"è¿›è¡Œå¿«é€Ÿæµ‹è¯•...")
                quick_test(model, iteration + 1)

        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_model_path = "pikazoo_selfplay_final"
        model.save(final_model_path)
        print(f"\nğŸ‰ è‡ªæˆ‘å¯¹æˆ˜è®­ç»ƒå®Œæˆï¼æœ€ç»ˆæ¨¡å‹ä¿å­˜ä¸º: {final_model_path}")

    except KeyboardInterrupt:
        print("\nè®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        interrupted_path = f"selfplay_checkpoints/pikazoo_selfplay_interrupted_{iteration}"
        model.save(interrupted_path)
        print(f"å½“å‰æ¨¡å‹å·²ä¿å­˜: {interrupted_path}")

    finally:
        env.close()

    return model


def quick_test(model, iteration):
    """å¿«é€Ÿæµ‹è¯•æ¨¡å‹è¡¨ç°"""
    test_env = SelfPlayGymWrapper()

    wins = 0
    total_games = 5

    for game in range(total_games):
        obs, info = test_env.reset()
        total_reward = 0

        for step in range(1000):  # æœ€å¤§æ­¥æ•°é™åˆ¶
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = test_env.step(action)
            total_reward += reward

            if terminated or truncated:
                if total_reward > 0:
                    wins += 1
                break

    win_rate = wins / total_games
    print(f"å¿«é€Ÿæµ‹è¯•ç»“æœ (è¿­ä»£ {iteration}): {wins}/{total_games} èƒœåˆ© (èƒœç‡: {win_rate:.1%})")

    test_env.close()


def test_selfplay_models():
    """æµ‹è¯•è‡ªæˆ‘å¯¹æˆ˜è®­ç»ƒçš„æ¨¡å‹"""
    print("æµ‹è¯•è‡ªæˆ‘å¯¹æˆ˜æ¨¡å‹...")

    # å°è¯•åŠ è½½æ¨¡å‹
    model_paths = [
        "pikazoo_selfplay_final",
        "selfplay_checkpoints/pikazoo_selfplay_iter_9",
        "selfplay_checkpoints/pikazoo_selfplay_iter_8"
    ]

    model = None
    for path in model_paths:
        try:
            model = PPO.load(path)
            print(f"æˆåŠŸåŠ è½½æ¨¡å‹: {path}")
            break
        except:
            continue

    if model is None:
        print("æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹ï¼è¯·å…ˆè¿è¡Œè®­ç»ƒã€‚")
        return

    # åˆ›å»ºæµ‹è¯•ç¯å¢ƒï¼ˆå¸¦å¯è§†åŒ–ï¼‰
    test_env = pikazoo_v0.env(winning_score=5, render_mode="human")
    test_env = SimplifyAction(test_env)

    print("å¼€å§‹AIå¯¹æˆ˜æµ‹è¯•ï¼ˆAI vs éšæœºå¯¹æ‰‹ï¼‰...")

    wins = {"ai": 0, "random": 0}

    for game in range(3):
        print(f"\n=== ç¬¬ {game + 1} åœºæµ‹è¯• ===")
        obs, info = test_env.reset()

        while test_env.agents:
            actions = {}

            # AI æ§åˆ¶ player_1
            action, _ = model.predict(obs["player_1"], deterministic=True)
            actions["player_1"] = action

            # éšæœºç­–ç•¥æ§åˆ¶ player_2
            actions["player_2"] = test_env.action_space("player_2").sample()

            obs, rewards, terms, truncs, infos = test_env.step(actions)

            if any(terms.values()):
                if rewards.get("player_1", 0) > 0:
                    wins["ai"] += 1
                    print("ğŸ¤– AI è·èƒœï¼")
                else:
                    wins["random"] += 1
                    print("ğŸ² éšæœºå¯¹æ‰‹è·èƒœï¼")
                break

    print(f"\n=== æœ€ç»ˆæµ‹è¯•ç»“æœ ===")
    print(f"AI è·èƒœ: {wins['ai']} åœº")
    print(f"éšæœºå¯¹æ‰‹è·èƒœ: {wins['random']} åœº")

    test_env.close()


if __name__ == "__main__":
    import sys

    if len(sys.argv) > 1 and sys.argv[1] == "test":
        test_selfplay_models()
    else:
        model = train_selfplay()

        # è®­ç»ƒå®Œæˆåè¯¢é—®æ˜¯å¦æµ‹è¯•
        test_choice = input("\nè‡ªæˆ‘å¯¹æˆ˜è®­ç»ƒå®Œæˆï¼æ˜¯å¦ç«‹å³æµ‹è¯•æ¨¡å‹ï¼Ÿ(y/n): ")
        if test_choice.lower() == 'y':
            test_selfplay_models()